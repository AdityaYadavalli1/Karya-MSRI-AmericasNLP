Could not read wav
Could not read bribri000494.wav
Could not read wav
Could not read bribri000744.wav
Downloading and preparing dataset csv/default to /home/t-hdiddee/.cache/huggingface/datasets/csv/default-2caf56cc49fd2773/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...
Dataset csv downloaded and prepared to /home/t-hdiddee/.cache/huggingface/datasets/csv/default-2caf56cc49fd2773/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.
49
{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'y': 23, 'z': 24, 'à': 25, 'á': 26, 'â': 27, 'è': 28, 'é': 29, 'ê': 30, 'ë': 31, 'ì': 32, 'í': 33, 'î': 34, 'ñ': 35, 'ò': 36, 'ó': 37, 'ô': 38, 'ö': 39, 'ù': 40, 'ú': 41, 'û': 42, '̀': 43, '́': 44, '̂': 45, '̠': 46, '|': 0, '[UNK]': 47, '[PAD]': 48, '<s>': 49, '</s>': 50}
51
51
Using custom data configuration default-2caf56cc49fd2773
Downloading data files: 100%|██████████████████████████| 2/2 [00:00<00:00, 8586.09it/s]
Extracting data files: 100%|███████████████████████████| 2/2 [00:00<00:00, 1985.00it/s]
0 tables [00:00, ? tables/s]/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/datasets/download/streaming_download_manager.py:697: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'
  return pd.read_csv(xopen(filepath_or_buffer, "rb", use_auth_token=use_auth_token), **kwargs)
0 tables [00:00, ? tables/s]/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/datasets/download/streaming_download_manager.py:697: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'
  return pd.read_csv(xopen(filepath_or_buffer, "rb", use_auth_token=use_auth_token), **kwargs)
100%|██████████████████████████████████████████████████| 2/2 [00:00<00:00, 1140.84it/s]
100%|█████████████████████████████████████████████| 495/495 [00:00<00:00, 23638.63ex/s]
100%|█████████████████████████████████████████████| 250/250 [00:00<00:00, 25048.40ex/s]
100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00, 495.84ba/s]
100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00, 674.22ba/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|                                                          | 0/495 [00:00<?, ?ex/s]/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
100%|███████████████████████████████████████████████| 495/495 [00:00<00:00, 849.25ex/s]
100%|██████████████████████████████████████████████| 250/250 [00:00<00:00, 1221.33ex/s]
Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?
Only 879 unigrams passed as vocabulary. Is this small or artificial data?
{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'y': 23, 'z': 24, 'à': 25, 'á': 26, 'â': 27, 'è': 28, 'é': 29, 'ê': 30, 'ë': 31, 'ì': 32, 'í': 33, 'î': 34, 'ñ': 35, 'ò': 36, 'ó': 37, 'ô': 38, 'ö': 39, 'ù': 40, 'ú': 41, 'û': 42, '̀': 43, '́': 44, '̂': 45, '̠': 46, '|': 0, '[UNK]': 47, '[PAD]': 48, '<s>': 49, '</s>': 50}
51
Some weights of the model checkpoint at facebook/wav2vec2-xls-r-300m were not used when initializing Wav2Vec2ForCTC: ['project_hid.weight', 'quantizer.codevectors', 'project_q.bias', 'project_q.weight', 'quantizer.weight_proj.weight', 'project_hid.bias', 'quantizer.weight_proj.bias']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1618: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.
  warnings.warn(
Using cuda_amp half precision backend
The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 495
  Num Epochs = 80
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 1200
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|                                                 | 2/1200 [00:04<39:21,  1.97s/it]/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(



























  4%|█▉                                              | 49/1200 [01:10<28:59,  1.51s/it]
  4%|██                                              | 50/1200 [01:11<25:11,  1.31s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8


 97%|████████████████████████████████████████████████▍ | 31/32 [00:04<00:00,  6.75it/s]
(250, 751, 51)
(0.873636744966443, 1.0452334630350195) are the lm CER and WER respectively.
1.0
1.0



























  8%|███▉                                           | 100/1200 [03:38<27:34,  1.50s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8
 19%|█████████▌                                         | 6/32 [00:00<00:03,  8.10it/s]



 97%|████████████████████████████████████████████████▍ | 31/32 [00:04<00:00,  6.63it/s]
Configuration saved in Bribri/checkpoint-100/config.json
Model weights saved in Bribri/checkpoint-100/pytorch_model.bin
Feature extractor saved in Bribri/checkpoint-100/preprocessor_config.json
(0.9280620805369127, 0.9669260700389105) are the lm CER and WER respectively.
1.0
1.0
{'eval_loss': 3.664231061935425, 'eval_wer': 1.0, 'eval_cer': 1.0, 'eval_runtime': 13.7155, 'eval_samples_per_second': 18.227, 'eval_steps_per_second': 2.333, 'epoch': 6.65}
Deleting older checkpoint [Bribri/checkpoint-200] due to args.save_total_limit
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(























 12%|█████▉                                         | 150/1200 [05:04<18:39,  1.07s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8
 12%|██████▍                                            | 4/32 [00:00<00:03,  8.10it/s]



 97%|████████████████████████████████████████████████▍ | 31/32 [00:04<00:00,  6.52it/s]
(250, 751, 51)
(0.9077181208053692, 0.9683852140077821) are the lm CER and WER respectively.
1.0
1.0


























 17%|███████▊                                       | 200/1200 [06:30<26:30,  1.59s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8
 22%|███████████▏                                       | 7/32 [00:00<00:03,  7.94it/s]


 97%|████████████████████████████████████████████████▍ | 31/32 [00:04<00:00,  6.82it/s]
(250, 751, 51)
(0.8813968120805369, 0.9747081712062257) are the lm CER and WER respectively.
1.0
1.0
Configuration saved in Bribri/checkpoint-200/config.json
Model weights saved in Bribri/checkpoint-200/pytorch_model.bin
Feature extractor saved in Bribri/checkpoint-200/preprocessor_config.json
Deleting older checkpoint [Bribri/checkpoint-100] due to args.save_total_limit
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
























 21%|█████████▊                                     | 250/1200 [07:56<21:44,  1.37s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8
  0%|                                                           | 0/32 [00:00<?, ?it/s]



 97%|████████████████████████████████████████████████▍ | 31/32 [00:04<00:00,  6.75it/s]
(250, 751, 51)
(0.8917785234899329, 0.97568093385214) are the lm CER and WER respectively.
1.0
1.0































 25%|███████████▋                                   | 299/1200 [09:18<19:24,  1.29s/it]
 25%|███████████▊                                   | 300/1200 [09:18<15:31,  1.03s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8


 97%|████████████████████████████████████████████████▍ | 31/32 [00:04<00:00,  6.74it/s]
(250, 751, 51)

Configuration saved in Bribri/checkpoint-300/config.json
1.0
1.0
{'eval_loss': 3.246135711669922, 'eval_wer': 1.0, 'eval_cer': 1.0, 'eval_runtime': 11.3784, 'eval_samples_per_second': 21.972, 'eval_steps_per_second': 2.812, 'epoch': 19.97}
Model weights saved in Bribri/checkpoint-300/pytorch_model.bin
Feature extractor saved in Bribri/checkpoint-300/preprocessor_config.json
Deleting older checkpoint [Bribri/checkpoint-200] due to args.save_total_limit
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(





















 29%|█████████████▋                                 | 348/1200 [10:42<18:52,  1.33s/it]
 29%|█████████████▋                                 | 350/1200 [10:45<21:21,  1.51s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8



 97%|████████████████████████████████████████████████▍ | 31/32 [00:04<00:00,  6.81it/s]
(250, 751, 51)
(0.8183724832214765, 0.9212062256809338) are the lm CER and WER respectively.

Configuration saved in Bribri//best_cer_model/config.json31/32 [00:04<00:00,  6.81it/s]
Model weights saved in Bribri//best_cer_model/pytorch_model.bin
Feature extractor saved in Bribri//best_cer_model/preprocessor_config.json
 29%|█████████████▏                               | 351/1200 [11:12<2:09:04,  9.12s/it]
0.972630033557047
0.972630033557047























 33%|███████████████▋                               | 400/1200 [12:22<21:36,  1.62s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8
{'loss': 2.33, 'learning_rate': 0.0002673333333333333, 'epoch': 26.65}


 97%|████████████████████████████████████████████████▍ | 31/32 [00:04<00:00,  6.63it/s]

Configuration saved in Bribri//best_cer_model/config.json31/32 [00:04<00:00,  6.63it/s]
(0.5201342281879194, 0.8516536964980544) are the lm CER and WER respectively.
Replacing existing best model w.r.t to CER
0.506606543624161
0.506606543624161
{'eval_loss': 2.0454649925231934, 'eval_wer': 0.9489299610894941, 'eval_cer': 0.506606543624161, 'eval_runtime': 26.4372, 'eval_samples_per_second': 9.456, 'eval_steps_per_second': 1.21, 'epoch': 26.65}
Model weights saved in Bribri//best_cer_model/pytorch_model.bin
Feature extractor saved in Bribri//best_cer_model/preprocessor_config.json
 33%|███████████████▋                               | 400/1200 [12:49<21:36,  1.62s/it]Saving model checkpoint to Bribri/checkpoint-400
Configuration saved in Bribri/checkpoint-400/config.json
Model weights saved in Bribri/checkpoint-400/pytorch_model.bin
Feature extractor saved in Bribri/checkpoint-400/preprocessor_config.json
Deleting older checkpoint [Bribri/checkpoint-300] due to args.save_total_limit
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(






















 38%|█████████████████▋                             | 450/1200 [14:01<11:41,  1.07it/s]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8

 69%|██████████████████████████████████▍               | 22/32 [00:03<00:01,  5.09it/s]

 97%|████████████████████████████████████████████████▍ | 31/32 [00:04<00:00,  6.71it/s]

Configuration saved in Bribri//best_cer_model/config.json31/32 [00:04<00:00,  6.71it/s]
(0.40761325503355705, 0.896887159533074) are the lm CER and WER respectively.
Replacing existing best model w.r.t to CER
Model weights saved in Bribri//best_cer_model/pytorch_model.bin
Feature extractor saved in Bribri//best_cer_model/preprocessor_config.json
 38%|█████████████████▋                             | 450/1200 [14:26<11:41,  1.07it/s]
0.4459941275167785
0.4459941275167785

























 42%|███████████████████▌                           | 500/1200 [15:40<15:53,  1.36s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8

 53%|██████████████████████████▌                       | 17/32 [00:02<00:02,  6.30it/s]


 97%|████████████████████████████████████████████████▍ | 31/32 [00:04<00:00,  6.80it/s]

Configuration saved in Bribri//best_cer_model/config.json31/32 [00:04<00:00,  6.80it/s]
(0.404991610738255, 0.811284046692607) are the lm CER and WER respectively.
Replacing existing best model w.r.t to CER
Model weights saved in Bribri//best_cer_model/pytorch_model.bin
Feature extractor saved in Bribri//best_cer_model/preprocessor_config.json
 42%|███████████████████▌                           | 500/1200 [16:04<15:53,  1.36s/it]Saving model checkpoint to Bribri/checkpoint-500
Configuration saved in Bribri/checkpoint-500/config.json
Model weights saved in Bribri/checkpoint-500/pytorch_model.bin
Feature extractor saved in Bribri/checkpoint-500/preprocessor_config.json
0.41243708053691275
0.41243708053691275
{'eval_loss': 1.9452018737792969, 'eval_wer': 0.8715953307392996, 'eval_cer': 0.41243708053691275, 'eval_runtime': 24.1612, 'eval_samples_per_second': 10.347, 'eval_steps_per_second': 1.324, 'epoch': 33.32}
Deleting older checkpoint [Bribri/checkpoint-400] due to args.save_total_limit
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(



























 46%|█████████████████████▌                         | 550/1200 [17:20<18:07,  1.67s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8
 62%|███████████████████████████████▎                  | 20/32 [00:02<00:02,  5.33it/s]

 97%|████████████████████████████████████████████████▍ | 31/32 [00:05<00:00,  6.59it/s]
(250, 751, 51)
(0.4157927852348993, 0.8253891050583657) are the lm CER and WER respectively.
0.41359060402684567
0.41243708053691275































 50%|███████████████████████▌                       | 600/1200 [18:39<10:42,  1.07s/it]
 50%|███████████████████████▌                       | 600/1200 [18:39<10:42,  1.07s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8


 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████    | 31/32 [00:05<00:00,  6.65it/s]
(250, 751, 51)

Configuration saved in Bribri//best_cer_model/config.json████████████████████████████████████████████████████████████████████████    | 31/32 [00:05<00:00,  6.65it/s]
Replacing existing best model w.r.t to CER
0.4044672818791946
0.4044672818791946
{'eval_loss': 2.096519947052002, 'eval_wer': 0.8667315175097277, 'eval_cer': 0.4044672818791946, 'eval_runtime': 24.6371, 'eval_samples_per_second': 10.147, 'eval_steps_per_second': 1.299, 'epoch': 39.97}
Model weights saved in Bribri//best_cer_model/pytorch_model.bin
Feature extractor saved in Bribri//best_cer_model/preprocessor_config.json
 50%|███████████████████████▌                       | 600/1200 [19:04<10:42,  1.07s/it]                                                                              Saving model checkpoint to Bribri/checkpoint-600
Configuration saved in Bribri/checkpoint-600/config.json
Model weights saved in Bribri/checkpoint-600/pytorch_model.bin
Feature extractor saved in Bribri/checkpoint-600/preprocessor_config.json
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(





























 54%|█████████████████████████▍                     | 650/1200 [20:21<13:50,  1.51s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8
 84%|████▏| 27/32 [00:04<00:01,  4.73it/s]

 97%|████▊| 31/32 [00:04<00:00,  6.78it/s]
(250, 751, 51)
(0.41568791946308725, 0.8151750972762646) are the lm CER and WER respectively.
0.41117869127516776
0.4044672818791946



























 58%|███████████████████████████▍                   | 699/1200 [21:38<09:14,  1.11s/it]
 58%|███████████████████████████▍                   | 700/1200 [21:40<12:32,  1.51s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8



 97%|████████████████████████████████████████████████████████████████████████████████████████████   | 31/32 [00:05<00:00,  6.63it/s]
(250, 751, 51)
(0.41621224832214765, 0.8253891050583657) are the lm CER and WER respectively.
0.4107592281879195
0.4044672818791946
Configuration saved in Bribri/checkpoint-700/config.json
Model weights saved in Bribri/checkpoint-700/pytorch_model.bin
Feature extractor saved in Bribri/checkpoint-700/preprocessor_config.json
Deleting older checkpoint [Bribri/checkpoint-600] due to args.save_total_limit
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
























 62%|█████████████████████████████▍                 | 750/1200 [23:01<08:05,  1.08s/it]
 62%|█████████████████████████████▍                 | 750/1200 [23:01<08:05,  1.08s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8


 97%|████████████████████████████████████████████████████████████████████████████████████████████   | 31/32 [00:05<00:00,  4.94it/s]

Configuration saved in Bribri//best_cer_model/config.json████████████████████████████████████████   | 31/32 [00:05<00:00,  4.94it/s]
(0.4123322147651007, 0.8321984435797666) are the lm CER and WER respectively.
Replacing existing best model w.r.t to CER
0.3949244966442953
0.3949244966442953
{'eval_loss': 2.1792712211608887, 'eval_wer': 0.8433852140077821, 'eval_cer': 0.3949244966442953, 'eval_runtime': 28.5204, 'eval_samples_per_second': 8.766, 'eval_steps_per_second': 1.122, 'epoch': 49.97}
Model weights saved in Bribri//best_cer_model/pytorch_model.bin
Feature extractor saved in Bribri//best_cer_model/preprocessor_config.json

























 67%|███████████████████████████████▎               | 799/1200 [24:45<12:14,  1.83s/it]
 67%|███████████████████████████████▎               | 800/1200 [24:46<10:37,  1.59s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8


 97%|████████████████████████████████████████████████████████████████████████████████████████████   | 31/32 [00:05<00:00,  5.98it/s]
(250, 751, 51)
(0.42218959731543626, 0.8224708171206225) are the lm CER and WER respectively.
0.4046770134228188
0.3949244966442953

Configuration saved in Bribri/checkpoint-800/config.json
Model weights saved in Bribri/checkpoint-800/pytorch_model.bin
Feature extractor saved in Bribri/checkpoint-800/preprocessor_config.json
Deleting older checkpoint [Bribri/checkpoint-700] due to args.save_total_limit
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(


























 71%|█████████████████████████████████▎             | 849/1200 [26:08<06:07,  1.05s/it]
 71%|█████████████████████████████████▎             | 850/1200 [26:11<09:34,  1.64s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8



 97%|███████████▋| 31/32 [00:04<00:00,  6.88it/s]
(250, 751, 51)
(0.4017407718120805, 0.835603112840467) are the lm CER and WER respectively.

Configuration saved in Bribri//best_cer_model/config.json
0.3863255033557047
0.3863255033557047
{'eval_loss': 2.3264458179473877, 'eval_wer': 0.8438715953307393, 'eval_cer': 0.3863255033557047, 'eval_runtime': 24.8761, 'eval_samples_per_second': 10.05, 'eval_steps_per_second': 1.286, 'epoch': 56.65}
Model weights saved in Bribri//best_cer_model/pytorch_model.bin
Feature extractor saved in Bribri//best_cer_model/preprocessor_config.json























 75%|███████████████████████████████████▎           | 900/1200 [27:43<05:04,  1.01s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8
  6%|▊            | 2/32 [00:00<00:01, 16.10it/s]



 97%|███████████▋| 31/32 [00:04<00:00,  6.89it/s]
(250, 751, 51)
(0.4160025167785235, 0.8394941634241245) are the lm CER and WER respectively.
0.3924077181208054
0.3863255033557047
Configuration saved in Bribri/checkpoint-900/config.json
Model weights saved in Bribri/checkpoint-900/pytorch_model.bin
Feature extractor saved in Bribri/checkpoint-900/preprocessor_config.json
Deleting older checkpoint [Bribri/checkpoint-800] due to args.save_total_limit
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(




























 79%|█████████████████████████████████████▏         | 950/1200 [29:06<05:33,  1.34s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8
 25%|███▎         | 8/32 [00:00<00:02,  8.65it/s]


 97%|███████████▋| 31/32 [00:04<00:00,  6.89it/s]

(250, 751, 51)
(0.41726090604026844, 0.8409533073929961) are the lm CER and WER respectively.
0.3964974832214765
0.3863255033557047




























 83%|██████████████████████████████████████▎       | 1000/1200 [30:25<04:46,  1.43s/it]
 83%|██████████████████████████████████████▎       | 1000/1200 [30:25<04:46,  1.43s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8


 97%|███████████▋| 31/32 [00:04<00:00,  6.75it/s]
Configuration saved in Bribri/checkpoint-1000/config.json
Model weights saved in Bribri/checkpoint-1000/pytorch_model.bin
Feature extractor saved in Bribri/checkpoint-1000/preprocessor_config.json
(0.4087667785234899, 0.8458171206225681) are the lm CER and WER respectively.
0.3928271812080537
0.3863255033557047
{'eval_loss': 2.408034563064575, 'eval_wer': 0.8608949416342413, 'eval_cer': 0.3928271812080537, 'eval_runtime': 8.6954, 'eval_samples_per_second': 28.751, 'eval_steps_per_second': 3.68, 'epoch': 66.65}
Deleting older checkpoint [Bribri/checkpoint-900] due to args.save_total_limit
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(























 88%|████████████████████████████████████████▎     | 1050/1200 [31:47<02:42,  1.08s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8
  6%|▊            | 2/32 [00:00<00:01, 16.85it/s]



 97%|███████████▋| 31/32 [00:04<00:00,  6.68it/s]
(250, 751, 51)
(0.4047818791946309, 0.8472762645914397) are the lm CER and WER respectively.
0.3883179530201342
0.3863255033557047



























 92%|██████████████████████████████████████████▏   | 1100/1200 [33:09<02:38,  1.59s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8
 12%|█▋           | 4/32 [00:00<00:03,  8.43it/s]



 97%|███████████▋| 31/32 [00:04<00:00,  6.17it/s]
(250, 751, 51)
(0.4072986577181208, 0.8404669260700389) are the lm CER and WER respectively.
0.3884228187919463
0.3863255033557047
Configuration saved in Bribri/checkpoint-1100/config.json
Model weights saved in Bribri/checkpoint-1100/pytorch_model.bin
Feature extractor saved in Bribri/checkpoint-1100/preprocessor_config.json
Deleting older checkpoint [Bribri/checkpoint-1000] due to args.save_total_limit
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(


























 96%|████████████████████████████████████████████  | 1150/1200 [34:31<01:19,  1.60s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8
  0%|                                                 | 0/32 [00:00<?, ?it/s]



 97%|██████████████████████████████████████▊ | 31/32 [00:04<00:00,  6.76it/s]

Configuration saved in Bribri//best_cer_model/config.json04<00:00,  6.76it/s]
(0.40006291946308725, 0.835603112840467) are the lm CER and WER respectively.
Replacing existing best model w.r.t to CER
0.3834941275167785
0.3834941275167785
{'eval_loss': 2.4578471183776855, 'eval_wer': 0.8516536964980544, 'eval_cer': 0.3834941275167785, 'eval_runtime': 25.6051, 'eval_samples_per_second': 9.764, 'eval_steps_per_second': 1.25, 'epoch': 76.65}
Model weights saved in Bribri//best_cer_model/pytorch_model.bin
Feature extractor saved in Bribri//best_cer_model/preprocessor_config.json
























100%|█████████████████████████████████████████████▉| 1199/1200 [36:03<00:01,  1.26s/it]
100%|██████████████████████████████████████████████| 1200/1200 [36:04<00:00,  1.01s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8


 97%|██████████████████████████████████████▊ | 31/32 [00:04<00:00,  6.79it/s]
(250, 751, 51)
(0.4004823825503356, 0.8317120622568094) are the lm CER and WER respectively.

Configuration saved in Bribri//best_cer_model/config.json04<00:00,  6.79it/s]
0.38244546979865773
0.38244546979865773
{'eval_loss': 2.4582431316375732, 'eval_wer': 0.853112840466926, 'eval_cer': 0.38244546979865773, 'eval_runtime': 28.0148, 'eval_samples_per_second': 8.924, 'eval_steps_per_second': 1.142, 'epoch': 79.97}
Model weights saved in Bribri//best_cer_model/pytorch_model.bin
Feature extractor saved in Bribri//best_cer_model/preprocessor_config.json
100%|██████████████████████████████████████████████| 1200/1200 [36:32<00:00, Saving model checkpoint to Bribri/checkpoint-1200
Configuration saved in Bribri/checkpoint-1200/config.json
Model weights saved in Bribri/checkpoint-1200/pytorch_model.bin
Feature extractor saved in Bribri/checkpoint-1200/preprocessor_config.json
Deleting older checkpoint [Bribri/checkpoint-1100] due to args.save_total_limit
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from Bribri/checkpoint-500 (score: 1.9452018737792969).
100%|██████████████████████████████████████████████| 1200/1200 [36:35<00:00,  1.83s/it]
{'train_runtime': 2195.8323, 'train_samples_per_second': 18.034, 'train_steps_per_second': 0.546, 'train_loss': 1.9096045382817586, 'epoch': 79.97}