Could not read wav
Could not read bribri000494.wav
Could not read wav
Could not read bribri000744.wav
Downloading and preparing dataset csv/default to /home/t-hdiddee/.cache/huggingface/datasets/csv/default-61e0d9d0268a28b2/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...
Dataset csv downloaded and prepared to /home/t-hdiddee/.cache/huggingface/datasets/csv/default-61e0d9d0268a28b2/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.
49
{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'y': 23, 'z': 24, 'à': 25, 'á': 26, 'â': 27, 'è': 28, 'é': 29, 'ê': 30, 'ë': 31, 'ì': 32, 'í': 33, 'î': 34, 'ñ': 35, 'ò': 36, 'ó': 37, 'ô': 38, 'ö': 39, 'ù': 40, 'ú': 41, 'û': 42, '̀': 43, '́': 44, '̂': 45, '̠': 46, '|': 0, '[UNK]': 47, '[PAD]': 48, '<s>': 49, '</s>': 50}
51
51
Using custom data configuration default-61e0d9d0268a28b2
Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 8747.25it/s]
Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2107.69it/s]
0 tables [00:00, ? tables/s]/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/datasets/download/streaming_download_manager.py:697: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'
  return pd.read_csv(xopen(filepath_or_buffer, "rb", use_auth_token=use_auth_token), **kwargs)
0 tables [00:00, ? tables/s]/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/datasets/download/streaming_download_manager.py:697: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'
  return pd.read_csv(xopen(filepath_or_buffer, "rb", use_auth_token=use_auth_token), **kwargs)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1129.78it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [00:00<00:00, 22036.16ex/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:00<00:00, 25313.86ex/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 497.37ba/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 706.83ba/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|                                                                                                                                     | 0/495 [00:00<?, ?ex/s]/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 495/495 [00:00<00:00, 778.03ex/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:00<00:00, 1289.76ex/s]
Some weights of the model checkpoint at facebook/wav2vec2-xls-r-300m were not used when initializing Wav2Vec2ForCTC: ['project_hid.weight', 'project_hid.bias', 'quantizer.weight_proj.weight', 'project_q.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'quantizer.codevectors']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1618: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.
  warnings.warn(
Using cuda_amp half precision backend
The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 495
  Num Epochs = 80
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 1200
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|                                                                                                                                    | 0/1200 [00:00<?, ?it/s]/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(

  0%|▌                                                                                                                           | 5/1200 [00:08<29:23,  1.48s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8


 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████    | 31/32 [00:04<00:00,  6.52it/s]
[[[-1.955e-02  6.445e-02 -8.118e-02 ...  1.357e-02  1.313e-02 -1.409e-01]
  [-2.750e-02  3.090e-02 -1.011e-01 ... -3.427e-02  9.375e-02 -9.875e-02]
  [-1.452e-02  8.887e-02 -1.426e-01 ...  3.527e-03 -4.602e-02 -1.388e-01]
  ...
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]]
 [[ 5.398e-04  6.378e-02 -7.507e-02 ...  1.982e-02  2.982e-02 -1.144e-01]
  [-3.802e-02  2.251e-02 -8.734e-02 ... -2.196e-02  1.123e-01 -7.117e-02]
  [-5.536e-02  8.247e-03 -8.539e-02 ... -4.071e-02  9.521e-02 -6.519e-02]
  ...
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]]
 [[ 8.766e-03  1.285e-01 -3.387e-02 ...  4.214e-02 -3.998e-02 -1.859e-01]
  [-2.054e-02  4.071e-02 -9.656e-02 ... -3.131e-02  1.096e-01 -1.290e-01]
  [-2.373e-02  8.270e-02 -1.259e-01 ... -3.375e-02  7.404e-02 -1.508e-01]
  ...
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]]
 ...
 [[ 3.311e-02  1.151e-01 -4.565e-02 ...  5.338e-02 -8.447e-02 -1.671e-01]
  [-5.222e-03  2.574e-02 -8.051e-02 ... -1.487e-02  9.760e-02 -1.108e-01]
  [-3.506e-02  1.502e-02 -8.521e-02 ... -3.217e-02  9.888e-02 -8.569e-02]
  ...
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]]
 [[-2.270e-03  1.058e-01 -8.484e-02 ...  2.541e-02 -4.108e-02 -1.676e-01]
  [ 8.865e-03  7.733e-02 -1.329e-01 ... -2.336e-02  4.053e-02 -1.472e-01]
  [-3.906e-02  2.321e-02 -1.202e-01 ... -4.889e-02  1.376e-01 -6.525e-02]
  ...
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]]
 [[ 5.392e-02  1.317e-01 -1.106e-02 ...  7.208e-02 -9.100e-02 -1.697e-01]
  [-8.110e-03  2.580e-02 -8.942e-02 ... -5.811e-02  1.444e-01 -1.458e-01]
  [-1.380e-02  1.959e-02 -7.770e-02 ... -6.909e-02  1.304e-01 -1.514e-01]
  ...
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]
  [-1.000e+02 -1.000e+02 -1.000e+02 ... -1.000e+02 -1.000e+02 -1.000e+02]]]
1.0469798657718121
1.0



  1%|█                                                                                                                          | 10/1200 [00:23<45:11,  2.28s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8

 72%|█████████████████████████████████████████████████████████████████████████████████████████▊                                   | 23/32 [00:03<00:02,  3.75it/s]
[[[-2.1095e-03  7.4890e-02 -8.8013e-02 ...  5.6366e-02 -1.0544e-02
   -1.4856e-01]
  [-9.9792e-03  3.6774e-02 -1.1517e-01 ...  6.3362e-03  8.0322e-02
   -1.0785e-01]
  [-3.6430e-03  9.1858e-02 -1.4929e-01 ...  3.9001e-02 -5.4443e-02
   -1.3977e-01]
  ...
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]]
 [[ 1.4526e-02  7.8125e-02 -8.1970e-02 ...  6.6406e-02  4.3068e-03
   -1.2317e-01]
  [-2.2522e-02  2.6993e-02 -9.5886e-02 ...  1.5854e-02  1.0645e-01
   -7.6050e-02]
  [-4.0222e-02  1.1879e-02 -9.2957e-02 ... -5.6496e-03  9.0820e-02
   -6.8787e-02]
  ...
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]]
 [[ 1.9989e-02  1.3306e-01 -3.1952e-02 ...  7.7393e-02 -5.1361e-02
   -1.8579e-01]
  [-2.1362e-03  5.0140e-02 -1.0596e-01 ...  6.6948e-03  9.7534e-02
   -1.3928e-01]
  [-9.7504e-03  9.3018e-02 -1.3416e-01 ...  3.5324e-03  6.0242e-02
   -1.5723e-01]
  ...
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]]
 ...
 [[ 4.4556e-02  1.1823e-01 -4.6844e-02 ...  8.9172e-02 -9.0820e-02
   -1.6467e-01]
  [ 1.2863e-02  3.1982e-02 -9.1797e-02 ...  2.6947e-02  8.7158e-02
   -1.1755e-01]
  [-1.8661e-02  2.0737e-02 -9.4666e-02 ...  7.4272e-03  9.2712e-02
   -9.0576e-02]
  ...
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]]
 [[ 1.0284e-02  1.1353e-01 -8.7036e-02 ...  6.4697e-02 -5.2643e-02
   -1.6846e-01]
  [ 2.4292e-02  8.6548e-02 -1.4111e-01 ...  1.5121e-02  2.8305e-02
   -1.5222e-01]
  [-2.2507e-02  3.1433e-02 -1.2915e-01 ... -1.2856e-02  1.3184e-01
   -7.0374e-02]
  ...
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]]
 [[ 6.1127e-02  1.3684e-01 -5.9700e-03 ...  1.0510e-01 -9.4604e-02
   -1.6479e-01]
  [-2.3842e-03  3.3875e-02 -9.4543e-02 ... -1.4908e-02  1.3928e-01
   -1.4990e-01]
  [-8.9417e-03  2.6520e-02 -8.2214e-02 ... -2.6367e-02  1.2488e-01
   -1.5442e-01]
  ...
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02
   -1.0000e+02]
  [-1.0000e+02 -1.0000e+02 -1.0000e+02 ... -1.0000e+02 -1.0000e+02

  File "asr_baseline.py", line 227, in <module>███████████████████████████████████████████████████████████████████████████████    | 31/32 [00:04<00:00,  6.18it/s]
    trainer.train()
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/trainer.py", line 1521, in train
    return inner_training_loop(
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/trainer.py", line 1840, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/trainer.py", line 2065, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/trainer.py", line 2787, in evaluate
    output = eval_loop(
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/trainer.py", line 3072, in evaluation_loop
    metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))
  File "asr_baseline.py", line 107, in compute_metrics
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py", line 139, in batch_decode
    return self.tokenizer.batch_decode(*args, **kwargs)
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py", line 471, in batch_decode
    batch_decoded = [
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py", line 472, in <listcomp>
    self.decode(
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py", line 588, in decode
    return self._decode(
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py", line 391, in _decode
    if skip_special_tokens and token in self.all_special_ids:
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1274, in all_special_ids
    all_toks = self.all_special_tokens
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1250, in all_special_tokens
    all_toks = [str(s) for s in self.all_special_tokens_extended]
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1263, in all_special_tokens_extended
    set_attr = self.special_tokens_map_extended
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1238, in special_tokens_map_extended
    attr_value = getattr(self, "_" + attr)
KeyboardInterrupt