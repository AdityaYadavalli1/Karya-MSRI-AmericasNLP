Could not read wav
Could not read bribri000494.wav
Could not read wav
Could not read bribri000744.wav
Downloading and preparing dataset csv/default to /home/t-hdiddee/.cache/huggingface/datasets/csv/default-eaa97e28bf7f1c46/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...
Dataset csv downloaded and prepared to /home/t-hdiddee/.cache/huggingface/datasets/csv/default-eaa97e28bf7f1c46/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.
49
{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'y': 23, 'z': 24, 'à': 25, 'á': 26, 'â': 27, 'è': 28, 'é': 29, 'ê': 30, 'ë': 31, 'ì': 32, 'í': 33, 'î': 34, 'ñ': 35, 'ò': 36, 'ó': 37, 'ô': 38, 'ö': 39, 'ù': 40, 'ú': 41, 'û': 42, '̀': 43, '́': 44, '̂': 45, '̠': 46, '|': 0, '[UNK]': 47, '[PAD]': 48, '<s>': 49, '</s>': 50}
51
51
Using custom data configuration default-eaa97e28bf7f1c46
Downloading data files: 100%|███████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 8346.87it/s]
Extracting data files: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2051.00it/s]
0 tables [00:00, ? tables/s]/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/datasets/download/streaming_download_manager.py:697: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'
  return pd.read_csv(xopen(filepath_or_buffer, "rb", use_auth_token=use_auth_token), **kwargs)
0 tables [00:00, ? tables/s]/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/datasets/download/streaming_download_manager.py:697: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'
  return pd.read_csv(xopen(filepath_or_buffer, "rb", use_auth_token=use_auth_token), **kwargs)
100%|███████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1148.34it/s]
100%|██████████████████████████████████████████████████████████████████████████████████| 495/495 [00:00<00:00, 24150.90ex/s]
100%|██████████████████████████████████████████████████████████████████████████████████| 250/250 [00:00<00:00, 26254.44ex/s]
100%|████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 502.25ba/s]
100%|████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 699.40ba/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|                                                                                               | 0/495 [00:00<?, ?ex/s]/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████| 495/495 [00:00<00:00, 1063.15ex/s]
100%|███████████████████████████████████████████████████████████████████████████████████| 250/250 [00:00<00:00, 1362.98ex/s]
Some weights of the model checkpoint at facebook/wav2vec2-xls-r-300m were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'project_q.weight', 'quantizer.codevectors', 'project_q.bias', 'project_hid.weight']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1618: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.
  warnings.warn(
Using cuda_amp half precision backend
The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 495
  Num Epochs = 80
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 1200
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"

  0%|▏                                                                                     | 2/1200 [00:04<40:38,  2.04s/it]/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/trainer.py:1808: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(

  0%|▎                                                                                     | 5/1200 [00:08<29:51,  1.50s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8


 97%|████████████████████████████████████████████████████████████████████████████████████▎  | 31/32 [00:04<00:00,  6.42it/s]
9575250
1.0644924496644295
1.0



  1%|▋                                                                                    | 10/1200 [00:24<45:51,  2.31s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8


 81%|██████████████████████████████████████████████████████████████████████▋                | 26/32 [00:04<00:01,  4.94it/s]


Configuration saved in Bribri//best_cer_model/config.json████████████████████████████████▎  | 31/32 [00:04<00:00,  6.80it/s]
Replacing existing best model w.r.t to CER
0.8872692953020134
0.8872692953020134
{'eval_loss': 11.697521209716797, 'eval_wer': 1.2641050583657587, 'eval_cer': 0.8872692953020134, 'eval_runtime': 24.8284, 'eval_samples_per_second': 10.069, 'eval_steps_per_second': 1.289, 'epoch': 0.65}
Model weights saved in Bribri//best_cer_model/pytorch_model.bin
Feature extractor saved in Bribri//best_cer_model/preprocessor_config.json
  1%|█                                                                                    | 15/1200 [00:54<59:01,  2.99s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8


 78%|███████████████████████████████████████████████████████████████████▉                   | 25/32 [00:03<00:01,  4.40it/s]


 97%|████████████████████████████████████████████████████████████████████████████████████▎  | 31/32 [00:04<00:00,  6.76it/s]
0.9774538590604027
0.8872692953020134



  2%|█▍                                                                                   | 20/1200 [01:10<45:31,  2.31s/it]The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 250
  Batch size = 8

  File "asr_baseline.py", line 231, in <module>██████████████████████████████████████████▎  | 31/32 [00:04<00:00,  6.66it/s]
    trainer.train()
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/trainer.py", line 1521, in train
    return inner_training_loop(
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/trainer.py", line 1840, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/trainer.py", line 2065, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/trainer.py", line 2787, in evaluate
    output = eval_loop(
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/trainer.py", line 3072, in evaluation_loop
    metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))
  File "asr_baseline.py", line 108, in compute_metrics
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py", line 139, in batch_decode
    return self.tokenizer.batch_decode(*args, **kwargs)
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py", line 471, in batch_decode
    batch_decoded = [
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py", line 472, in <listcomp>
    self.decode(
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py", line 588, in decode
    return self._decode(
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py", line 391, in _decode
    if skip_special_tokens and token in self.all_special_ids:
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1274, in all_special_ids
    all_toks = self.all_special_tokens
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1250, in all_special_tokens
    all_toks = [str(s) for s in self.all_special_tokens_extended]
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1263, in all_special_tokens_extended
    set_attr = self.special_tokens_map_extended
  File "/home/t-hdiddee/venv/asr/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1238, in special_tokens_map_extended
    attr_value = getattr(self, "_" + attr)
KeyboardInterrupt
9575250